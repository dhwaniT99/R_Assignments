---
title: "Project 2 - FDA - Task2"
author: "Ankita Ajit Doddihal, Dhwani Trivedi, Miraj Bhimani"
date: "12/13/2021"
output: html_document
---

```{r}
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(plotly)
library(janeaustenr)
library(igraph)
library(ggraph)
```

#Importing Data Sets
```{r}

df_2017 <- read.csv("~/Desktop/FDA Project 2/data/2017.csv")
df_2018 <- read.csv("~/Desktop/FDA Project 2/data/2018.csv")
df_2019 <- read.csv("~/Desktop/FDA Project 2/data/2019.csv")
df_2020 <- read.csv("~/Desktop/FDA Project 2/data/2020.csv")
df_2021 <- read.csv("~/Desktop/FDA Project 2/data/2021.csv")

```


#For 2017

```{r}
df_2017 <- separate(
  df_2017,
  date,
  into = c("Year", "Month", "Date&Time"),
  sep = "-",
  remove = FALSE, convert = TRUE, fill = "left"
)

df_2017
#Converting tweet text to lower case
df_2017$tweet <- tolower(df_2017$tweet)


#Selecting tweets only from the year of 2017 from the data set
tweets_2017 <- select(df_2017, Year, tweet)%>% filter(Year=="2017")
df_2017 =data.frame(format(as.Date(as.character(tweets_2017$Year), format="%Y"),df_2017$tweet))

tweets_2017

#Spliting the tweet into words and removing stop words

remove_reg <- "https?://[^\\s]+|&amp;|&lt;|&gt;|\bRT\\b|t.co|http|@|[0-9]"

words_2017 <- tweets_2017 %>% 
  filter(!str_detect(tweet, "^RT")) %>%
  mutate(tweet = str_remove_all(tweet, remove_reg)) %>%
  unnest_tokens(word, tweet) %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"))

##Task 2 - Part 1

#Counting the occurance of words
words_count_2017<- words_2017 %>%
  group_by(word) %>%
  count()%>%
  arrange(-n)

#Finding the total number of words
totalwords<-data.frame(sum(words_count_2017$n))

#Finding the frequency
words_count_2017<- data.frame(words_count_2017,totalwords)
frequency2017 <- words_count_2017$Frequency <- (words_count_2017$n/words_count_2017$sum.words_count_2017.n.)


#Rank
Words_2017 <- mutate(words_count_2017,frequency2017,rank=row_number())

##Task 2 - Part 2
top10_2017 <- head(words_count_2017,10)
top10_2017


##Task 2 - Part 3

top10_2017$word


fig <- plot_ly(top10_2017, type='bar', x = top10_2017$word , y = top10_2017$Frequency)
               
fig                         
#histogram_2017

##Task 2 - Part 4

ggplot(Words_2017,aes(rank,words_count_2017$Frequency)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
  
##Task 2 - Part 5


##Tokenizing by ngram
bigrams_2017 <- tweets_2017 %>%
 unnest_tokens(bigram, tweet, token = "ngrams", n = 2)
bigrams_2017

#Removing Stop Words

bigrams_2017<- bigrams_2017 %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_2017 <- bigrams_2017 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)%>%
  mutate(word1 = str_remove_all(word1, remove_reg))%>%
  mutate(word2 = str_remove_all(word2, remove_reg))
  
#Counting and filtering n-grams
count_bigrams_2017<- bigrams_2017 %>%
  count(word1, word2)%>%
  arrange(-n)%>%
  na_if("")%>%
  na.omit()

#Plotting the Bigram graph

bigram_graph_2017 <- count_bigrams_2017 %>%
  filter(n>2)%>%
  graph_from_data_frame()

arrow <- grid::arrow(type = "open", length = unit(.1, "inches"))

ggraph(bigram_graph_2017, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = arrow, end_cap = circle(.05, 'inches'), color="black") +
  geom_node_point(color = "pink", size = 3) +
  geom_node_text(aes(label = name), vjust = 0.7, hjust = 0.7) +
  theme_void()

```

#2018
```{r}
df_2018 <- separate(
  df_2018,
  date,
  into = c("Year", "Month", "Date&Time"),
  sep = "-",
  remove = FALSE, convert = TRUE, fill = "left"
)
#Converting tweet text to lower case
df_2018$tweet <- tolower(df_2018$tweet)


#Selecting tweets only from the year of 2017 from the data set
tweets_2018 <- select(df_2018, Year, tweet)%>% filter(Year=="2018")


#Spliting the tweet into words and removing stop words

remove_reg <- "https?://[^\\s]+|&amp;|&lt;|&gt;|\bRT\\b|t.co|http|@|[0-9]"

words_2018 <- tweets_2018 %>% 
  filter(!str_detect(tweet, "^RT")) %>%
  mutate(tweet = str_remove_all(tweet, remove_reg)) %>%
  unnest_tokens(word, tweet) %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"))

##Task 2 - Part 1

#Counting the occurance of words
words_count_2018<- words_2018 %>%
  group_by(word) %>%
  count()%>%
  arrange(-n)


#Finding the total number of words
totalwords<-data.frame(sum(words_count_2018$n))

#Finding the frequency
words_count_2018<- data.frame(words_count_2018,totalwords)
frequency2018 <- words_count_2018$Frequency <- (words_count_2018$n/words_count_2018$sum.words_count_2018.n.)

words_count_2018
#Rank

Words_2018 <- mutate(words_count_2018,frequency2018,rank=row_number())
##Task 2 - Part 2
top10_2018 <- head(words_count_2018,10)

##Task 2 - Part 3

fig_2018 <- plot_ly(top10_2018, type='bar', x = top10_2018$word , y = top10_2018$Frequency)
fig_2018


##Task 2 - Part 4
Words_2018 <- mutate(words_count_2018,frequency2018,rank=row_number())
ggplot(Words_2018,aes(rank,words_count_2018$Frequency)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

##Task 2 - Part 5

##Tokenizing by ngram
bigrams_2018 <- tweets_2018 %>%
 unnest_tokens(bigram, tweet, token = "ngrams", n = 2)
bigrams_2018

#Removing Stop Words

bigrams_2018<- bigrams_2018 %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_2018 <- bigrams_2018 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)%>%
  mutate(word1 = str_remove_all(word1, remove_reg))%>%
  mutate(word2 = str_remove_all(word2, remove_reg))
  
#Counting and filtering n-grams
count_bigrams_2018<- bigrams_2018 %>%
  count(word1, word2)%>%
  arrange(-n)%>%
  na_if("")%>%
  na.omit()

#Plotting the Bigram graph

bigram_graph_2018 <- count_bigrams_2018 %>%
  filter(n>4)%>%
  graph_from_data_frame()

arrow <- grid::arrow(type = "open", length = unit(.1, "inches"))

ggraph(bigram_graph_2018, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = arrow, end_cap = circle(.05, 'inches'), color="black") +
  geom_node_point(color = "pink", size = 3) +
  geom_node_text(aes(label = name), vjust = 0.5, hjust = 0.5) +
  theme_void()


```

```{r}
df_2019 <- separate(
  df_2019,
  date,
  into = c("Year", "Month", "Date&Time"),
  sep = "-",
  remove = FALSE, convert = TRUE, fill = "left"
)
#Converting tweet text to lower case
df_2019$tweet <- tolower(df_2019$tweet)


#Selecting tweets only from the year of 2017 from the data set
tweets_2019 <- select(df_2019, Year, tweet)%>% filter(Year=="2019")


#Spliting the tweet into words and removing stop words

remove_reg <- "https?://[^\\s]+|&amp;|&lt;|&gt;|\bRT\\b|t.co|http|@|[0-9]|"

words_2019<- tweets_2019 %>% 
  filter(!str_detect(tweet, "^RT")) %>%
  mutate(tweet = str_remove_all(tweet, remove_reg)) %>%
  unnest_tokens(word, tweet) %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"))

##Task 2 - Part 1

#Counting the occurance of words
words_count_2019<- words_2019 %>%
  group_by(word) %>%
  count()%>%
  arrange(-n)

#Finding the total number of words
totalwords<-data.frame(sum(words_count_2019$n))

#Finding the frequency
words_count_2019<- data.frame(words_count_2019,totalwords)
words_count_2019$Frequency <- (words_count_2019$n/words_count_2019$sum.words_count_2019.n.)

words_count_2019


##Task 2 - Part 2
top10_2019 <- head(words_count_2019,10)


##Task 2 - Part 3

fig_2019 <- plot_ly(top10_2019, type='bar', x = top10_2019$word , y = top10_2019$Frequency)
fig_2019
##Task 2 - Part 4
Words_2019 <- mutate(words_count_2019,words_count_2019$Frequency,rank=row_number())
ggplot(Words_2019,aes(rank,words_count_2019$Frequency)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()

##Task 2 - Part 5

##Tokenizing by ngram
bigrams_2019 <- tweets_2019 %>%
 unnest_tokens(bigram, tweet, token = "ngrams", n = 2)
bigrams_2019

#Removing Stop Words

bigrams_2019<- bigrams_2019 %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_2019 <- bigrams_2019%>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)%>%
  mutate(word1 = str_remove_all(word1, remove_reg))%>%
  mutate(word2 = str_remove_all(word2, remove_reg))
  
#Counting and filtering n-grams
count_bigrams_2019<- bigrams_2019 %>%
  count(word1, word2)%>%
  arrange(-n)%>%
  na_if("")%>%
  na.omit()

#Plotting the Bigram graph

bigram_graph_2019 <- count_bigrams_2019 %>%
  filter(n>5)%>%
  graph_from_data_frame()

arrow <- grid::arrow(type = "open", length = unit(.1, "inches"))

ggraph(bigram_graph_2019, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = arrow, end_cap = circle(.05, 'inches'), color="black") +
  geom_node_point(color = "pink", size = 3) +
  geom_node_text(aes(label = name), vjust = 0.7, hjust = 0.7) +
  theme_void()

```

#For 2020
```{r}
df_2020 <- separate(
  df_2020,
  date,
  into = c("Year", "Month", "Date&Time"),
  sep = "-",
  remove = FALSE, convert = TRUE, fill = "left"
)
#Converting tweet text to lower case
df_2020$tweet <- tolower(df_2020$tweet)


#Selecting tweets only from the year of 2017 from the data set
tweets_2020 <- select(df_2020, Year, tweet)%>% filter(Year=="2020")


#Spliting the tweet into words and removing stop words

remove_reg <- "https?://[^\\s]+|&amp;|&lt;|&gt;|\bRT\\b|t.co|http|@|[0-9]"

words_2020 <- tweets_2020 %>% 
  filter(!str_detect(tweet, "^RT")) %>%
  mutate(tweet = str_remove_all(tweet, remove_reg)) %>%
  unnest_tokens(word, tweet) %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"))

##Task 2 - Part 1

#Counting the occurance of words
words_count_2020<- words_2020 %>%
  group_by(word) %>%
  count()%>%
  arrange(-n)

#Finding the total number of words
totalwords<-data.frame(sum(words_count_2020$n))

#Finding the frequency
words_count_2020<- data.frame(words_count_2020,totalwords)
words_count_2020$Frequency <- (words_count_2020$n/words_count_2020$sum.words_count_2020.n.)

words_count_2020


##Task 2 - Part 2
top10_2020 <- head(words_count_2020,10)


##Task 2 - Part 3

fig_2020 <- plot_ly(top10_2020, type='bar', x = top10_2020$word , y = top10_2020$Frequency)
fig_2020

##Task 2 - Part 4

Words_2020 <- mutate(words_count_2020,words_count_2020$Frequency,rank=row_number())
ggplot(Words_2020,aes(rank,words_count_2020$Frequency)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()


##Task 2 - Part 5

##Tokenizing by ngram
bigrams_2020 <- tweets_2020 %>%
 unnest_tokens(bigram, tweet, token = "ngrams", n = 2)
bigrams_2020

#Removing Stop Words

bigrams_2020<- bigrams_2020 %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_2020 <- bigrams_2020 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)%>%
  mutate(word1 = str_remove_all(word1, remove_reg))%>%
  mutate(word2 = str_remove_all(word2, remove_reg))
  
#Counting and filtering n-grams
count_bigrams_2020<- bigrams_2020 %>%
  count(word1, word2)%>%
  arrange(-n)%>%
  na_if("")%>%
  na.omit()

#Plotting the Bigram graph

bigram_graph_2020 <- count_bigrams_2020 %>%
  filter(n>4)%>%
  graph_from_data_frame()

arrow <- grid::arrow(type = "open", length = unit(.1, "inches"))

ggraph(bigram_graph_2020, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = arrow, end_cap = circle(.05, 'inches'), color="black") +
  geom_node_point(color = "pink", size = 3) +
  geom_node_text(aes(label = name), vjust = 0.7, hjust = 0.7) +
  theme_void()

```
```{r}
df_2021 <- separate(
  df_2021,
  date,
  into = c("Year", "Month", "Date&Time"),
  sep = "-",
  remove = FALSE, convert = TRUE, fill = "left"
)
#Converting tweet text to lower case
df_2021$tweet <- tolower(df_2021$tweet)


#Selecting tweets only from the year of 2017 from the data set
tweets_2021 <- select(df_2021, Year, tweet)%>% filter(Year=="2021")


#Spliting the tweet into words and removing stop words

remove_reg <- "https?://[^\\s]+|&amp;|&lt;|&gt;|\bRT\\b|t.co|http|@|[0-9]"

words_2021 <- tweets_2021 %>% 
  filter(!str_detect(tweet, "^RT")) %>%
  mutate(tweet = str_remove_all(tweet, remove_reg)) %>%
  unnest_tokens(word, tweet) %>%
  filter(!word %in% stop_words$word,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"))

##Task 2 - Part 1

#Counting the occurance of words
words_count_2021<- words_2021 %>%
  group_by(word) %>%
  count()%>%
  arrange(-n)

#Finding the total number of words
totalwords<-data.frame(sum(words_count_2021$n))

#Finding the frequency
words_count_2021<- data.frame(words_count_2021,totalwords)
words_count_2021$Frequency <- (words_count_2021$n/words_count_2021$sum.words_count_2021.n.)

words_count_2021

##Task 2 - Part 2
top10_2021 <- head(words_count_2021,10)


##Task 2 - Part 3

fig_2021 <- plot_ly(top10_2021, type='bar', x = top10_2021$word , y = top10_2021$Frequency)
fig_2021

##Task 2 - Part 4

Words_2021 <- mutate(words_count_2021,words_count_2021$Frequency,rank=row_number())
ggplot(Words_2021,aes(rank,words_count_2021$Frequency)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()



##Task 2 - Part 5

##Tokenizing by ngram
bigrams_2021 <- tweets_2021 %>%
 unnest_tokens(bigram, tweet, token = "ngrams", n = 2)
bigrams_2021

#Removing Stop Words

bigrams_2021<- bigrams_2021 %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_2021 <- bigrams_2021 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)%>%
  mutate(word1 = str_remove_all(word1, remove_reg))%>%
  mutate(word2 = str_remove_all(word2, remove_reg))
  
#Counting and filtering n-grams
count_bigrams_2021<- bigrams_2021 %>%
  count(word1, word2)%>%
  arrange(-n)%>%
  na_if("")%>%
  na.omit()

#Plotting the Bigram graph

bigram_graph_2021 <- count_bigrams_2021 %>%
  filter(n>1)%>%
  graph_from_data_frame()

arrow <- grid::arrow(type = "open", length = unit(.1, "inches"))

ggraph(bigram_graph_2021, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = arrow, end_cap = circle(.05, 'inches')) +
  geom_node_point(color = "lightpink", size = 2) +
  geom_node_text(aes(label = name), vjust = 0.7, hjust = 0.7) +
  theme_void()

```

